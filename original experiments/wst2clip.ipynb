{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from kymatio.torch import Scattering2D\n",
    "import pywt\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import clip\n",
    "from transformers import AutoImageProcessor, AutoModel, AutoProcessor, AutoModelForImageClassification\n",
    "import torch.nn.functional as F\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, roc_curve\n",
    "from scipy.special import softmax\n",
    "\n",
    "# import cuml\n",
    "# import cudf\n",
    "import torchmetrics\n",
    "import torchmetrics.classification as tmc\n",
    "import torchmetrics.functional as tmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SRC_PATH = \"../Data/GenImage/\"\n",
    "generator_names = [\"adm\", \"bgan\", \"glide\", \"midj\", \"sd_14\", \"sd_15\", \"vqdm\", \"wukong\"]\n",
    "with open(\"classes.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "classes_idx = data[\"1k_idx\"]\n",
    "classes_names = data[\"21k_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fixed_pca(tensor, n_components=48):\n",
    "    # input: wst_tensor shape: (B, C=81, H, W)\n",
    "    B, C, H, W = tensor.shape\n",
    "    X = tensor.permute(0, 2, 3, 1).reshape(-1, C)  # shape: (B*H*W, C)\n",
    "    \n",
    "    # centralize\n",
    "    X_mean = X.mean(dim=0, keepdim=True)\n",
    "    X_centered = X - X_mean\n",
    "\n",
    "    # SVD（or torch.pca_lowrank）\n",
    "    U, S, Vh = torch.linalg.svd(X_centered, full_matrices=False)\n",
    "    Vh_reduced = Vh[:n_components]  # shape: (D, C)\n",
    "\n",
    "    return Vh_reduced, X_mean\n",
    "\n",
    "\n",
    "def apply_fixed_pca(tensor, Vh_reduced, X_mean):\n",
    "    # wst_tensor: (B, C=81, H, W)\n",
    "    B, C, H, W = tensor.shape\n",
    "    X = tensor.permute(0, 2, 3, 1).reshape(-1, C)              # (B*H*W, C)\n",
    "    X_centered = X - X_mean[None, :]                              # (B*H*W, C)\n",
    "    X_reduced = X_centered @ Vh_reduced.T                         # (B*H*W, D)\n",
    "    X_reduced_reshaped = X_reduced.view(B, H, W, -1).permute(0, 3, 1, 2)  # (B, D, H, W)\n",
    "    \n",
    "    return X_reduced_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_short_side(img, target_short_side):\n",
    "    w, h = img.size\n",
    "    if w < h:\n",
    "        new_w = target_short_side\n",
    "        new_h = int(h * (target_short_side / w))\n",
    "    else:\n",
    "        new_h = target_short_side\n",
    "        new_w = int(w * (target_short_side / h))\n",
    "    return img.resize((new_w, new_h), Image.BILINEAR)\n",
    "\n",
    "wst_shape = (256, 256)\n",
    "wst_preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Lambda(lambda img: resize_short_side(img, min(wst_shape))),  \n",
    "        transforms.CenterCrop(wst_shape),  \n",
    "        transforms.ToTensor(),  \n",
    "    ]\n",
    ")\n",
    "\n",
    "def normalize_image(tensor):\n",
    "\n",
    "    B, C, H, W = tensor.shape\n",
    "    tensor_flat = tensor.view(B, -1)  # (B, C*H*W)\n",
    "    min_val = tensor_flat.min(dim=1, keepdim=True)[0]\n",
    "    max_val = tensor_flat.max(dim=1, keepdim=True)[0]\n",
    "    normalized = (tensor_flat - min_val) / (max_val - min_val + 1e-8)\n",
    "    return normalized.view(B, C, H, W)\n",
    "\n",
    "\n",
    "def normalize_channel(tensor):\n",
    "\n",
    "    B, C, H, W = tensor.shape\n",
    "    tensor_flat = tensor.view(B, C, -1)  # (B, C, H*W)\n",
    "    min_val = tensor_flat.min(dim=2, keepdim=True)[0]\n",
    "    max_val = tensor_flat.max(dim=2, keepdim=True)[0]\n",
    "    normalized = (tensor_flat - min_val) / (max_val - min_val + 1e-8)\n",
    "    return normalized.view(B, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSTDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(path).convert(\"L\")\n",
    "            image = self.transform(image)\n",
    "            return image, str(path)\n",
    "        except:\n",
    "            print(\"Failure open image.\")\n",
    "            return None\n",
    "\n",
    "def wst_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None, None\n",
    "    images, paths = zip(*batch)\n",
    "    return torch.stack(images), paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wst_transform(input_path, transform, J=2, batch_size = 64, device = device):\n",
    "    input_dir = Path(input_path)\n",
    "    image_paths = list(input_dir.glob(\"*\"))\n",
    "    scattering = Scattering2D(J=J, shape=wst_shape).to(device)\n",
    "    dataset = WSTDataset(image_paths, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=wst_collate_fn)\n",
    "    wst_results = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch_images, batch_paths = batch\n",
    "            batch_images = batch_images.to(device)  # shape: [batch_size, 1, H, W]\n",
    "            coeffs = scattering(batch_images)  #  [batch_size, C, C', H', W']\n",
    "            coeffs = coeffs.squeeze(1)\n",
    "            wst_results.append(coeffs.cpu())\n",
    "    wst_tensor = torch.cat(wst_results, dim=0)\n",
    "\n",
    "    return wst_tensor\n",
    "\n",
    "def reshape_to_3(x_reduced):\n",
    "    # x_reduced: (B, D=48, 64, 64)\n",
    "    B, D, H, W = x_reduced.shape\n",
    "    assert D == 48\n",
    "    x_split = x_reduced.view(B, 3, 16, H, W)  # (B, 3, 16, 64, 64)\n",
    "    x_reshaped = rearrange(x_split, 'B c (h w) H W -> B c (h H) (w W)', h=4, w=4)\n",
    "    return x_reshaped  # (B, 3, 256, 256)\n",
    "\n",
    "def prepare_for_clip_batch(wst_tensor: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    # Step 1: Resize to (B, 3, 224, 224)\n",
    "    resized = F.interpolate(wst_tensor, size=224, mode='bicubic', align_corners=False)\n",
    "\n",
    "    # Step 2: Normalize（ broadcast to batch normalize）\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=wst_tensor.device).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=wst_tensor.device).view(1, 3, 1, 1)\n",
    "\n",
    "    normalized = (resized - mean) / std  #  broadcast to batch\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_pipeline(input_path, transform, cls, device = device):\n",
    "    wst_tensor = wst_transform(input_path, transform).to(device)\n",
    "    wst_tensor = normalize_image(wst_tensor)\n",
    "    Vh, mean = compute_fixed_pca(wst_tensor, n_components=48)\n",
    "    torch.save({'Vh': Vh, 'mean': mean}, f'../Data/Features/wst_pca/{cls}.pt')\n",
    "    wst_reduced = apply_fixed_pca(wst_tensor, Vh, mean)\n",
    "    model_input = reshape_to_3(wst_reduced)\n",
    "    return model_input.cpu(), Vh.cpu(), mean.cpu()\n",
    "\n",
    "def compute_pipeline(input_path, transform, Vh, mean, device = device):\n",
    "    wst_tensor = wst_transform(input_path, transform).to(device)\n",
    "    wst_tensor = normalize_image(wst_tensor)\n",
    "    wst_reduced = apply_fixed_pca(wst_tensor, Vh.to(device), mean.to(device))\n",
    "    model_input = reshape_to_3(wst_reduced)\n",
    "    return model_input.cpu()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_encode(input_tensor, model, batch_size):\n",
    "    dataset = TensorDataset(input_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    all_features = []\n",
    "    for batch in tqdm(dataloader, desc=\"Image Encoding\"):\n",
    "        tensors = batch[0].to(device)  # (batch_size, 3, 256, 256)\n",
    "\n",
    "        # Resize + Normalize\n",
    "        tensors = prepare_for_clip_batch(tensors)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feats = model.encode_image(tensors)\n",
    "\n",
    "        all_features.append(feats.cpu())\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_compute(gt_tensors, eps=1e-3):\n",
    "    embeddings = gt_tensors.to(device)\n",
    "    mean = embeddings.mean(dim=0, keepdim=True)\n",
    "    X = embeddings - mean\n",
    "    cov = X.T @ X / (embeddings.size(0) - 1)\n",
    "    cov += eps * torch.eye(cov.size(0), device=embeddings.device)\n",
    "    return mean, cov\n",
    "\n",
    "\n",
    "def mahalanobis_distance(x, mean, cov):\n",
    "    x = x.to(torch.float32).view(-1)\n",
    "    mean = mean.to(torch.float32).view(-1)\n",
    "    delta = x - mean\n",
    "    cov = cov.to(torch.float32)\n",
    "\n",
    "    try:\n",
    "        sol = torch.linalg.solve(cov, delta.unsqueeze(1))  # [D, 1]\n",
    "        dist_squared = delta @ sol.squeeze()\n",
    "        if dist_squared < 0:\n",
    "            print(\"Warning: distance squared < 0\", dist_squared.item())\n",
    "            dist_squared = torch.clamp(dist_squared, min=0.0)\n",
    "        dist = torch.sqrt(dist_squared)\n",
    "        return dist\n",
    "    except RuntimeError as e:\n",
    "        print(\"Runtime error in Mahalanobis:\", e)\n",
    "        return torch.tensor(float(\"nan\"), device=x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padim_detector(input_path, cls, model, batch_size = 64, device = device):\n",
    "    data = {\"AUROC\": {}, \"AUPRC\": {}, \"FPR95\":{}}\n",
    "    gt_tensor, Vh, mean = pca_pipeline(input_path + \"/nature\", wst_preprocess, cls)\n",
    "    gt_features = clip_encode(gt_tensor, model, batch_size)\n",
    "    gt_mean, gt_cov = gt_compute(gt_features)\n",
    "    gt_mean, gt_cov = gt_mean.to(device), gt_cov.to(device)\n",
    "    features = {}\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\", \"nature_2\"]:\n",
    "        all_tensor = compute_pipeline(input_path + f\"/{generator}\", wst_preprocess, Vh, mean)\n",
    "        all_features = clip_encode(all_tensor, model, batch_size)\n",
    "        features[generator] = all_features\n",
    "\n",
    "    bgan_m = torch.cat([features[\"bgan\"], features[\"nature_2\"]], dim=0).to(device)\n",
    "    midj_m = torch.cat([features[\"midj\"], features[\"nature_2\"]], dim=0).to(device)\n",
    "    sd_15_m = torch.cat([features[\"sd_15\"], features[\"nature_2\"]], dim=0).to(device)\n",
    "    labels = np.concatenate((np.zeros(features[\"bgan\"].shape[0]), np.ones(features[\"nature_2\"].shape[0])))\n",
    "    mixed = [bgan_m, midj_m, sd_15_m]\n",
    "    for idx, generator in enumerate([\"bgan\", \"midj\", \"sd_15\"]):\n",
    "        save_path = f\"../Data/Padim_results/wst2/{cls}/{generator}.png\"\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        test_scores = []\n",
    "        for sample in mixed[idx]:\n",
    "            distance = mahalanobis_distance(sample, gt_mean, gt_cov)\n",
    "            test_scores.append(-distance.cpu())\n",
    "        scores = np.array(test_scores)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "        idx = np.where(tpr >= 0.95)[0][0]\n",
    "        fpr_95 = fpr[idx]\n",
    "        distances = np.sqrt((1 - tpr) ** 2 + fpr**2)\n",
    "        best_threshold = thresholds[np.argmin(distances)]\n",
    "        print(\"Best threshold(ROC):\", best_threshold)\n",
    "\n",
    "        roc_auc = roc_auc_score(labels, scores)\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        ax1.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "        ax1.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "        ax1.set_xlim([0.0, 1.0])\n",
    "        ax1.set_ylim([0.0, 1.05])\n",
    "        ax1.set_xlabel(\"False Positive Rate (FPR)\")\n",
    "        ax1.set_ylabel(\"True Positive Rate (TPR)\")\n",
    "        ax1.set_title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "\n",
    "        precision, recall, thresholds = precision_recall_curve(labels, scores)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        # print(\"AUPRC:\", pr_auc)\n",
    "\n",
    "        ax2.plot(recall, precision, color=\"blue\", lw=2, label=f\"PR curve (area = {pr_auc:.2f})\")\n",
    "        ax2.set_xlim([0.0, 1.0])\n",
    "        ax2.set_ylim([0.0, 1.05])\n",
    "        ax2.set_xlabel(\"Recall\")\n",
    "        ax2.set_ylabel(\"Precision\")\n",
    "        ax2.set_title(\"Precision-Recall Curve\")\n",
    "        ax2.legend(loc=\"best\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "        data[\"AUROC\"][generator] = roc_auc\n",
    "        data[\"AUPRC\"][generator] = pr_auc\n",
    "        data[\"FPR95\"][generator] = fpr_95\n",
    "    \n",
    "    return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "dict_all = {}\n",
    "for cls in classes_idx:\n",
    "    data = padim_detector(f\"../Data/GenImage/{cls}\", cls, model)\n",
    "    dict_all[cls] = data\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for class_name, data in dict_all.items(): \n",
    "    for generator, _ in data[\"AUROC\"].items():\n",
    "        row = {\n",
    "            \"CLASS\": class_name,\n",
    "            \"GENERATOR\": generator,\n",
    "            \"AUROC\": data[\"AUROC\"][generator],\n",
    "            \"AUPRC\": data[\"AUPRC\"][generator],\n",
    "            \"FPR95\": data[\"FPR95\"][generator],\n",
    "        }\n",
    "        all_rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(all_rows)\n",
    "df.to_csv(\"wst_clip_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bgan</th>\n",
       "      <th>midj</th>\n",
       "      <th>sd_15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AUPRC</th>\n",
       "      <td>0.497237</td>\n",
       "      <td>0.561964</td>\n",
       "      <td>0.644344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUROC</th>\n",
       "      <td>0.457205</td>\n",
       "      <td>0.550591</td>\n",
       "      <td>0.632697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FPR95</th>\n",
       "      <td>0.970370</td>\n",
       "      <td>0.897531</td>\n",
       "      <td>0.880247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            bgan      midj     sd_15\n",
       "metric                              \n",
       "AUPRC   0.497237  0.561964  0.644344\n",
       "AUROC   0.457205  0.550591  0.632697\n",
       "FPR95   0.970370  0.897531  0.880247"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_df = pd.read_csv(\"wst_clip_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
