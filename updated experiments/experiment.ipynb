{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IPTP\\Stage_3A\\Stage\\Code\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from kymatio.torch import Scattering2D\n",
    "import pywt\n",
    "from collections import defaultdict\n",
    "import clip\n",
    "from transformers import AutoImageProcessor, AutoModel, AutoProcessor, AutoModelForImageClassification\n",
    "\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import Encoder\n",
    "from wst2vit import clip_encoder, dinov2_encoder\n",
    "import time\n",
    "from rpo import SparseProjector, GaussianProjector\n",
    "import utils\n",
    "from eval import Metrics\n",
    "from padim import mahalanobis_detector, padim_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patchcore import greedy_coreset_selection, coreset_detector, patchcore_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"classes.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "classes_idx = data[\"1k_idx\"]\n",
    "classes_names = data[\"21k_idx\"]\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding:   0%|          | 0/2 [00:00<?, ?it/s]d:\\IPTP\\Stage_3A\\Stage\\Code\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:06<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:06<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:06<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:06<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:06<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:06<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:06<00:00,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:06<00:00,  3.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:06<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clip Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clip_encoder = Encoder(\"clip\")\n",
    "for cls in classes_idx:\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\", \"nature\", \"nature_2\"]:\n",
    "        input_path = f\"../Data/GenImage/{cls}/{generator}\"\n",
    "        save_path_clip = f\"../Data/Features/clip/{cls}/{generator}.pt\"\n",
    "        os.makedirs(os.path.dirname(save_path_clip), exist_ok=True)\n",
    "        tensor = clip_encoder(input_path)\n",
    "        torch.save(tensor, save_path_clip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Dinov2 Patches Encoding:   0%|          | 0/2 [00:00<?, ?it/s]d:\\IPTP\\Stage_3A\\Stage\\Code\\.venv\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:81: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:07<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:08<00:00,  4.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:07<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:07<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:08<00:00,  4.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Patches Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 256, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# clip_patch_encoder = Encoder(\"clip\", patchify = True)\n",
    "dinov2_patch_encoder = Encoder(\"dinov2\", patchify = True)\n",
    "# tc = {\"clip\": [], \"dinov2\": []}\n",
    "for cls in classes_idx:\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\", \"nature\", \"nature_2\"]:\n",
    "        input_path = f\"../Data/GenImage/{cls}/{generator}\"\n",
    "        # save_path_clip = f\"../Data/Features/clip_patch/{cls}/{generator}.pt\"\n",
    "        save_path_dinov2 = f\"../Data/Features/dinov2_patch/{cls}/{generator}.pt\"\n",
    "        # os.makedirs(os.path.dirname(save_path_clip), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(save_path_dinov2), exist_ok=True)\n",
    "        # start = time.time()\n",
    "        # tensor1 = clip_patch_encoder(input_path)\n",
    "        # end = time.time()\n",
    "        # tc[\"clip\"].append(float(end - start))\n",
    "        # torch.save(tensor1, save_path_clip)\n",
    "        # start = time.time()\n",
    "        tensor2 = dinov2_patch_encoder(input_path)\n",
    "        # end = time.time()\n",
    "        # tc[\"dinov2\"].append(float(end - start))\n",
    "        torch.save(tensor2, save_path_dinov2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_time = np.mean(tc[\"clip\"])\n",
    "dinov2_time = np.mean(tc[\"dinov2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2608294677734375 3.895852737426758\n"
     ]
    }
   ],
   "source": [
    "print(clip_time, dinov2_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:08<00:00,  4.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:09<00:00,  4.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:09<00:00,  4.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:09<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:09<00:00,  4.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:09<00:00,  4.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:09<00:00,  4.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:08<00:00,  4.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:09<00:00,  4.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:09<00:00,  4.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dinov2 Direct Encoding: 100%|██████████| 2/2 [00:03<00:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dinov2_mean_encoder = Encoder(\"dinov2\")\n",
    "tc = {\"dinov2\": []}\n",
    "for cls in classes_idx:\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\", \"nature\", \"nature_2\"]:\n",
    "        input_path = f\"../Data/GenImage/{cls}/{generator}\"\n",
    "        save_path_dinov2 = f\"../Data/Features/dinov2_mean/{cls}/{generator}.pt\"\n",
    "        os.makedirs(os.path.dirname(save_path_dinov2), exist_ok=True)\n",
    "        start = time.time()\n",
    "        tensor2 = dinov2_mean_encoder(input_path)\n",
    "        end = time.time()\n",
    "        tc[\"dinov2\"].append(float(end - start))\n",
    "        torch.save(tensor2, save_path_dinov2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.94347270488739\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(tc[\"dinov2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:05<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:05<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:05<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:05<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:05<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:05<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:05<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:05<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:04<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:05<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:05<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:05<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:05<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:00<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-2 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 81, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WST-3 Direct Encoding: 100%|██████████| 2/2 [00:01<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 217, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "wst2_encoder = Encoder(\"wst\", J=2)\n",
    "wst3_encoder = Encoder(\"wst\", J=3)\n",
    "tc = {\"wst2\": [], \"wst3\": []}\n",
    "for cls in classes_idx:\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\", \"nature\", \"nature_2\"]:\n",
    "        input_path = f\"../Data/GenImage/{cls}/{generator}\"\n",
    "        save_path_wst2 = f\"../Data/Features/wst2/{cls}/{generator}.pt\"\n",
    "        save_path_wst3 = f\"../Data/Features/wst3/{cls}/{generator}.pt\"\n",
    "        os.makedirs(os.path.dirname(save_path_wst2), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(save_path_wst3), exist_ok=True)\n",
    "        start = time.time()\n",
    "        tensor1 = wst2_encoder(input_path)\n",
    "        end = time.time()\n",
    "        tc[\"wst2\"].append(float(end - start))\n",
    "        torch.save(tensor1, save_path_wst2)\n",
    "        start = time.time()\n",
    "        tensor2 = wst3_encoder(input_path)\n",
    "        end = time.time()\n",
    "        tc[\"wst3\"].append(float(end - start))\n",
    "        torch.save(tensor2, save_path_wst3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0060737657547 2.12442666053772\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(tc[\"wst2\"]), np.mean(tc[\"wst3\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip, Dinov2-mean recalculation with Mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 354.41it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 533.22it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 522.17it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 505.11it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 533.59it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 553.04it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 503.18it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 512.24it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 509.44it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 506.21it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 489.71it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 470.90it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 498.86it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 522.41it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 509.15it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 528.17it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 521.71it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 506.13it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 503.96it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 501.68it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 525.21it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 513.63it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 496.86it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 488.74it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 485.58it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 520.63it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 524.86it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 512.26it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 528.70it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 535.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip mahalanobis auroc: 0.5141365645480872, auprc: 0.512197574958008, fpr95: 0.8090534979423866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = {\"CLASS\": [], \"GENERATOR\": [], \"AUROC\": [], \"AUPRC\": [], \"FPR95\": []}\n",
    "for cls in classes_idx:\n",
    "    gt_tensor = torch.load(f\"../Data/Features/clip/{cls}/nature.pt\",weights_only = True).to(DEVICE)\n",
    "    real_tensor = torch.load(f\"../Data/Features/clip/{cls}/nature_2.pt\", weights_only = True).to(DEVICE)\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\"]:\n",
    "        fake_tensor = torch.load(f\"../Data/Features/clip/{cls}/{generator}.pt\", weights_only = True).to(DEVICE)\n",
    "        labels = np.concatenate((np.zeros(real_tensor.shape[0]), np.ones(fake_tensor.shape[0])))\n",
    "        scores = mahalanobis_detector(gt_tensor, real_tensor, fake_tensor)\n",
    "        m = Metrics(labels, scores)\n",
    "        r,p,f = m.computation()\n",
    "        data[\"CLASS\"].append(cls)\n",
    "        data[\"GENERATOR\"].append(generator)\n",
    "        data[\"AUROC\"].append(r)\n",
    "        data[\"AUPRC\"].append(p)\n",
    "        data[\"FPR95\"].append(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"results_v2/clip_mahalanobis.csv\", index=False)\n",
    "print(f\"clip mahalanobis auroc: {np.mean(data['AUROC'])}, auprc: {np.mean(data['AUPRC'])}, fpr95: {np.mean(data['FPR95'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:01<00:00, 311.39it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 448.36it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 443.89it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 451.80it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 439.50it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 430.63it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 438.21it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 439.61it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 435.68it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 427.94it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 436.21it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 436.82it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 461.27it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 453.52it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 432.72it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 451.89it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 460.99it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 448.91it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 436.66it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 444.43it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 439.56it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 444.46it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 441.56it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 446.52it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 446.40it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 437.03it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 442.98it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 451.71it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 445.55it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 459.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dinov2 mean mahalanobis auroc: 0.6954757912919779, auprc: 0.6718992831746406, fpr95: 0.6034979423868312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = {\"CLASS\": [], \"GENERATOR\": [], \"AUROC\": [], \"AUPRC\": [], \"FPR95\": []}\n",
    "for cls in classes_idx:\n",
    "    gt_tensor = torch.load(f\"../Data/Features/dinov2_mean/{cls}/nature.pt\",weights_only = True).to(DEVICE)\n",
    "    real_tensor = torch.load(f\"../Data/Features/dinov2_mean/{cls}/nature_2.pt\", weights_only = True).to(DEVICE)\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\"]:\n",
    "        fake_tensor = torch.load(f\"../Data/Features/dinov2_mean/{cls}/{generator}.pt\", weights_only = True).to(DEVICE)\n",
    "        labels = np.concatenate((np.zeros(real_tensor.shape[0]), np.ones(fake_tensor.shape[0])))\n",
    "        scores = mahalanobis_detector(gt_tensor, real_tensor, fake_tensor)\n",
    "        m = Metrics(labels, scores)\n",
    "        r,p,f = m.computation()\n",
    "        data[\"CLASS\"].append(cls)\n",
    "        data[\"GENERATOR\"].append(generator)\n",
    "        data[\"AUROC\"].append(r)\n",
    "        data[\"AUPRC\"].append(p)\n",
    "        data[\"FPR95\"].append(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"results_v2/dinov2_mean_mahalanobis.csv\", index=False)\n",
    "print(f\"dinov2 mean mahalanobis auroc: {np.mean(data['AUROC'])}, auprc: {np.mean(data['AUPRC'])}, fpr95: {np.mean(data['FPR95'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip, Dinov2, WST-3 recalculation with Memory Coreset and PatchCore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Coreset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip coreset auroc: 0.5391962607326118, auprc: 0.532540669358274, fpr95: 0.8164609053497943\n"
     ]
    }
   ],
   "source": [
    "data = {\"CLASS\": [], \"GENERATOR\": [], \"AUROC\": [], \"AUPRC\": [], \"FPR95\": []}\n",
    "for cls in classes_idx:\n",
    "    gt_tensor = torch.load(f\"../Data/Features/clip/{cls}/nature.pt\",weights_only = True).to(DEVICE)\n",
    "    real_tensor = torch.load(f\"../Data/Features/clip/{cls}/nature_2.pt\", weights_only = True).to(DEVICE)\n",
    "    memory_coreset, m_idx = greedy_coreset_selection(gt_tensor, l=0.5)\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\"]:\n",
    "        fake_tensor = torch.load(f\"../Data/Features/clip/{cls}/{generator}.pt\", weights_only = True).to(DEVICE)\n",
    "        labels = np.concatenate((np.zeros(real_tensor.shape[0]), np.ones(fake_tensor.shape[0])))\n",
    "        scores = coreset_detector(memory_coreset, real_tensor, fake_tensor)\n",
    "        m = Metrics(labels, scores)\n",
    "        r,p,f = m.computation()\n",
    "        data[\"CLASS\"].append(cls)\n",
    "        data[\"GENERATOR\"].append(generator)\n",
    "        data[\"AUROC\"].append(r)\n",
    "        data[\"AUPRC\"].append(p)\n",
    "        data[\"FPR95\"].append(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"results_v2/clip_coreset.csv\", index=False)\n",
    "print(f\"clip coreset auroc: {np.mean(data['AUROC'])}, auprc: {np.mean(data['AUPRC'])}, fpr95: {np.mean(data['FPR95'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dinov2 mean coreset auroc: 0.7117309353248997, auprc: 0.6806247266903488, fpr95: 0.6123456790123457\n"
     ]
    }
   ],
   "source": [
    "data = {\"CLASS\": [], \"GENERATOR\": [], \"AUROC\": [], \"AUPRC\": [], \"FPR95\": []}\n",
    "for cls in classes_idx:\n",
    "    gt_tensor = torch.load(f\"../Data/Features/dinov2_mean/{cls}/nature.pt\",weights_only = True).to(DEVICE)\n",
    "    real_tensor = torch.load(f\"../Data/Features/dinov2_mean/{cls}/nature_2.pt\", weights_only = True).to(DEVICE)\n",
    "    memory_coreset, m_idx = greedy_coreset_selection(gt_tensor, l=0.5)\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\"]:\n",
    "        fake_tensor = torch.load(f\"../Data/Features/dinov2_mean/{cls}/{generator}.pt\", weights_only = True).to(DEVICE)\n",
    "        labels = np.concatenate((np.zeros(real_tensor.shape[0]), np.ones(fake_tensor.shape[0])))\n",
    "        scores = coreset_detector(memory_coreset, real_tensor, fake_tensor)\n",
    "        m = Metrics(labels, scores)\n",
    "        r,p,f = m.computation()\n",
    "        data[\"CLASS\"].append(cls)\n",
    "        data[\"GENERATOR\"].append(generator)\n",
    "        data[\"AUROC\"].append(r)\n",
    "        data[\"AUPRC\"].append(p)\n",
    "        data[\"FPR95\"].append(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"results_v2/dinov2_mean_coreset.csv\", index=False)\n",
    "print(f\"dinov2 mean coreset auroc: {np.mean(data['AUROC'])}, auprc: {np.mean(data['AUPRC'])}, fpr95: {np.mean(data['FPR95'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wst3 sparse coreset auroc: 0.44994157394706086, auprc: 0.4743008738162199, fpr95: 0.9211934156378601\n"
     ]
    }
   ],
   "source": [
    "data = {\"CLASS\": [], \"GENERATOR\": [], \"AUROC\": [], \"AUPRC\": [], \"FPR95\": []}\n",
    "for cls in classes_idx:\n",
    "    proj_s = SparseProjector(217, 16)\n",
    "    gt_tensor = torch.load(f\"../Data/Features/wst3/{cls}/nature.pt\", weights_only = True).to(DEVICE)\n",
    "    real_tensor = torch.load(f\"../Data/Features/wst3/{cls}/nature_2.pt\", weights_only = True).to(DEVICE)\n",
    "    gt_tensor = torch.nn.functional.adaptive_avg_pool2d(gt_tensor, output_size=(16, 16))\n",
    "    real_tensor = torch.nn.functional.adaptive_avg_pool2d(real_tensor, output_size=(16, 16))\n",
    "    gt_tensor = utils.wst_m(gt_tensor, proj_s)\n",
    "    real_tensor = utils.wst_m(real_tensor, proj_s)\n",
    "    memory_coreset, m_idx = greedy_coreset_selection(gt_tensor, l=0.5)\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\"]:\n",
    "        fake_tensor = torch.load(f\"../Data/Features/wst3/{cls}/{generator}.pt\", weights_only = True).to(DEVICE)\n",
    "        fake_tensor = torch.nn.functional.adaptive_avg_pool2d(fake_tensor, output_size=(16, 16))\n",
    "        fake_tensor = utils.wst_m(fake_tensor, proj_s)\n",
    "        labels = np.concatenate((np.zeros(real_tensor.shape[0]), np.ones(fake_tensor.shape[0])))\n",
    "        scores = coreset_detector(memory_coreset, real_tensor, fake_tensor)\n",
    "        m = Metrics(labels, scores)\n",
    "        r,p,f = m.computation()\n",
    "        data[\"CLASS\"].append(cls)\n",
    "        data[\"GENERATOR\"].append(generator)\n",
    "        data[\"AUROC\"].append(r)\n",
    "        data[\"AUPRC\"].append(p)\n",
    "        data[\"FPR95\"].append(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"results_v2/wst3_sparse_coreset.csv\", index=False)\n",
    "print(f\"wst3 sparse coreset auroc: {np.mean(data['AUROC'])}, auprc: {np.mean(data['AUPRC'])}, fpr95: {np.mean(data['FPR95'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatchCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip patchcore auroc: 0.38810839302951794, auprc: 0.42072097432556643, fpr95: 0.9390946502057613\n"
     ]
    }
   ],
   "source": [
    "data = {\"CLASS\": [], \"GENERATOR\": [], \"AUROC\": [], \"AUPRC\": [], \"FPR95\": []}\n",
    "projector = SparseProjector(768,100)\n",
    "for cls in classes_idx:\n",
    "    gt_patches = torch.load(f\"../Data/Features/clip_patch/{cls}/nature.pt\",weights_only = True).to(DEVICE)\n",
    "    real_patches = torch.load(f\"../Data/Features/clip_patch/{cls}/nature_2.pt\", weights_only = True).to(DEVICE)\n",
    "    gt_patches = gt_patches.reshape(-1, gt_patches.shape[-1])\n",
    "    gt_patches_proj = projector.project(gt_patches)\n",
    "    coreset, indices = greedy_coreset_selection(gt_patches_proj, l=0.5)\n",
    "    memory_coreset = gt_patches[indices]\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\"]:\n",
    "        fake_patches = torch.load(f\"../Data/Features/clip_patch/{cls}/{generator}.pt\", weights_only = True).to(DEVICE)\n",
    "        labels = np.concatenate((np.zeros(real_patches.shape[0]), np.ones(fake_patches.shape[0])))\n",
    "        scores = patchcore_detector(memory_coreset, real_patches, fake_patches)\n",
    "        m = Metrics(labels, scores)\n",
    "        r,p,f = m.computation()\n",
    "        data[\"CLASS\"].append(cls)\n",
    "        data[\"GENERATOR\"].append(generator)\n",
    "        data[\"AUROC\"].append(r)\n",
    "        data[\"AUPRC\"].append(p)\n",
    "        data[\"FPR95\"].append(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"results_v2/clip_patchcore.csv\", index=False)\n",
    "print(f\"clip patchcore auroc: {np.mean(data['AUROC'])}, auprc: {np.mean(data['AUPRC'])}, fpr95: {np.mean(data['FPR95'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dinov2 patchcore auroc: 0.5911281308743586, auprc: 0.5762045610478104, fpr95: 0.7818930041152262\n"
     ]
    }
   ],
   "source": [
    "data = {\"CLASS\": [], \"GENERATOR\": [], \"AUROC\": [], \"AUPRC\": [], \"FPR95\": []}\n",
    "projector = SparseProjector(768,100)\n",
    "for cls in classes_idx:\n",
    "    gt_patches = torch.load(f\"../Data/Features/dinov2_patch/{cls}/nature.pt\",weights_only = True).to(DEVICE)\n",
    "    real_patches = torch.load(f\"../Data/Features/dinov2_patch/{cls}/nature_2.pt\", weights_only = True).to(DEVICE)\n",
    "    gt_patches = gt_patches.reshape(-1, gt_patches.shape[-1])\n",
    "    gt_patches_proj = projector.project(gt_patches)\n",
    "    coreset, indices = greedy_coreset_selection(gt_patches_proj, l=0.5)\n",
    "    memory_coreset = gt_patches[indices]\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\"]:\n",
    "        fake_patches = torch.load(f\"../Data/Features/dinov2_patch/{cls}/{generator}.pt\", weights_only = True).to(DEVICE)\n",
    "        labels = np.concatenate((np.zeros(real_patches.shape[0]), np.ones(fake_patches.shape[0])))\n",
    "        scores = patchcore_detector(memory_coreset, real_patches, fake_patches)\n",
    "        m = Metrics(labels, scores)\n",
    "        r,p,f = m.computation()\n",
    "        data[\"CLASS\"].append(cls)\n",
    "        data[\"GENERATOR\"].append(generator)\n",
    "        data[\"AUROC\"].append(r)\n",
    "        data[\"AUPRC\"].append(p)\n",
    "        data[\"FPR95\"].append(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"results_v2/dinov2_patchcore.csv\", index=False)\n",
    "print(f\"dinov2 patchcore auroc: {np.mean(data['AUROC'])}, auprc: {np.mean(data['AUPRC'])}, fpr95: {np.mean(data['FPR95'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WST 2&3 -Mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wst2 mahalanobis auroc: 0.4254648681603414, auprc: 0.47431262034051974, fpr95: 0.9049382716049384\n"
     ]
    }
   ],
   "source": [
    "data = {\"CLASS\": [], \"GENERATOR\": [], \"AUROC\": [], \"AUPRC\": [], \"FPR95\": []}\n",
    "for cls in classes_idx:\n",
    "    proj_s = SparseProjector(81, 8)\n",
    "    gt_tensor = torch.load(f\"../Data/Features/wst2/{cls}/nature.pt\", weights_only = True).to(DEVICE)\n",
    "    real_tensor = torch.load(f\"../Data/Features/wst2/{cls}/nature_2.pt\", weights_only = True).to(DEVICE)\n",
    "    gt_tensor = torch.nn.functional.adaptive_avg_pool2d(gt_tensor, output_size=(16, 16))\n",
    "    real_tensor = torch.nn.functional.adaptive_avg_pool2d(real_tensor, output_size=(16, 16))\n",
    "    gt_tensor = utils.wst_m(gt_tensor, proj_s)\n",
    "    real_tensor = utils.wst_m(real_tensor, proj_s)\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\"]:\n",
    "        fake_tensor = torch.load(f\"../Data/Features/wst2/{cls}/{generator}.pt\", weights_only = True).to(DEVICE)\n",
    "        fake_tensor = torch.nn.functional.adaptive_avg_pool2d(fake_tensor, output_size=(16, 16))\n",
    "        fake_tensor = utils.wst_m(fake_tensor, proj_s)\n",
    "        labels = np.concatenate((np.zeros(real_tensor.shape[0]), np.ones(fake_tensor.shape[0])))\n",
    "        scores = mahalanobis_detector(gt_tensor, real_tensor, fake_tensor)\n",
    "        m = Metrics(labels, scores)\n",
    "        r,p,f = m.computation()\n",
    "        data[\"CLASS\"].append(cls)\n",
    "        data[\"GENERATOR\"].append(generator)\n",
    "        data[\"AUROC\"].append(r)\n",
    "        data[\"AUPRC\"].append(p)\n",
    "        data[\"FPR95\"].append(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"results_v2/wst2_sparse_mahalanobis.csv\", index=False)\n",
    "print(f\"wst2 mahalanobis auroc: {np.mean(data['AUROC'])}, auprc: {np.mean(data['AUPRC'])}, fpr95: {np.mean(data['FPR95'])}\")\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wst3 sparse mahalanobis auroc: 0.4263742823756541, auprc: 0.48064977056659886, fpr95: 0.8965020576131687\n"
     ]
    }
   ],
   "source": [
    "data = {\"CLASS\": [], \"GENERATOR\": [], \"AUROC\": [], \"AUPRC\": [], \"FPR95\": []}\n",
    "for cls in classes_idx:\n",
    "    proj_s = SparseProjector(217, 16)\n",
    "    gt_tensor = torch.load(f\"../Data/Features/wst3/{cls}/nature.pt\", weights_only = True).to(DEVICE)\n",
    "    real_tensor = torch.load(f\"../Data/Features/wst3/{cls}/nature_2.pt\", weights_only = True).to(DEVICE)\n",
    "    gt_tensor = torch.nn.functional.adaptive_avg_pool2d(gt_tensor, output_size=(16, 16))\n",
    "    real_tensor = torch.nn.functional.adaptive_avg_pool2d(real_tensor, output_size=(16, 16))\n",
    "    gt_tensor = utils.wst_m(gt_tensor, proj_s)\n",
    "    real_tensor = utils.wst_m(real_tensor, proj_s)\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\"]:\n",
    "        fake_tensor = torch.load(f\"../Data/Features/wst3/{cls}/{generator}.pt\", weights_only = True).to(DEVICE)\n",
    "        fake_tensor = torch.nn.functional.adaptive_avg_pool2d(fake_tensor, output_size=(16, 16))\n",
    "        fake_tensor = utils.wst_m(fake_tensor, proj_s)\n",
    "        labels = np.concatenate((np.zeros(real_tensor.shape[0]), np.ones(fake_tensor.shape[0])))\n",
    "        scores = mahalanobis_detector(gt_tensor, real_tensor, fake_tensor)\n",
    "        m = Metrics(labels, scores)\n",
    "        r,p,f = m.computation()\n",
    "        data[\"CLASS\"].append(cls)\n",
    "        data[\"GENERATOR\"].append(generator)\n",
    "        data[\"AUROC\"].append(r)\n",
    "        data[\"AUPRC\"].append(p)\n",
    "        data[\"FPR95\"].append(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"results_v2/wst3_sparse_mahalanobis.csv\", index=False)\n",
    "print(f\"wst3 sparse mahalanobis auroc: {np.mean(data['AUROC'])}, auprc: {np.mean(data['AUPRC'])}, fpr95: {np.mean(data['FPR95'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wst2 gaussian mahalanobis auroc: 0.42860844383478136, auprc: 0.4762146218749842, fpr95: 0.905349794238683\n"
     ]
    }
   ],
   "source": [
    "data = {\"CLASS\": [], \"GENERATOR\": [], \"AUROC\": [], \"AUPRC\": [], \"FPR95\": []}\n",
    "for cls in classes_idx:\n",
    "    proj_g = GaussianProjector(81,8)\n",
    "    gt_tensor = torch.load(f\"../Data/Features/wst2/{cls}/nature.pt\", weights_only = True).to(DEVICE)\n",
    "    real_tensor = torch.load(f\"../Data/Features/wst2/{cls}/nature_2.pt\", weights_only = True).to(DEVICE)\n",
    "    gt_tensor = torch.nn.functional.adaptive_avg_pool2d(gt_tensor, output_size=(16, 16))\n",
    "    real_tensor = torch.nn.functional.adaptive_avg_pool2d(real_tensor, output_size=(16, 16))\n",
    "    gt_tensor = utils.wst_m(gt_tensor, proj_g)\n",
    "    real_tensor = utils.wst_m(real_tensor, proj_g)\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\"]:\n",
    "        fake_tensor = torch.load(f\"../Data/Features/wst2/{cls}/{generator}.pt\", weights_only = True).to(DEVICE)\n",
    "        fake_tensor = torch.nn.functional.adaptive_avg_pool2d(fake_tensor, output_size=(16, 16))\n",
    "        fake_tensor = utils.wst_m(fake_tensor, proj_g)\n",
    "        labels = np.concatenate((np.zeros(real_tensor.shape[0]), np.ones(fake_tensor.shape[0])))\n",
    "        scores = mahalanobis_detector(gt_tensor, real_tensor, fake_tensor)\n",
    "        m = Metrics(labels, scores)\n",
    "        r,p,f = m.computation()\n",
    "        data[\"CLASS\"].append(cls)\n",
    "        data[\"GENERATOR\"].append(generator)\n",
    "        data[\"AUROC\"].append(r)\n",
    "        data[\"AUPRC\"].append(p)\n",
    "        data[\"FPR95\"].append(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"results_v2/wst2_gaussian_mahalanobis.csv\", index=False)\n",
    "print(f\"wst2 gaussian mahalanobis auroc: {np.mean(data['AUROC'])}, auprc: {np.mean(data['AUPRC'])}, fpr95: {np.mean(data['FPR95'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wst3 gaussian mahalanobis auroc: 0.42946959304984006, auprc: 0.4792573234349718, fpr95: 0.8997942386831274\n"
     ]
    }
   ],
   "source": [
    "data = {\"CLASS\": [], \"GENERATOR\": [], \"AUROC\": [], \"AUPRC\": [], \"FPR95\": []}\n",
    "for cls in classes_idx:\n",
    "    proj_g = GaussianProjector(217,16)\n",
    "    gt_tensor = torch.load(f\"../Data/Features/wst3/{cls}/nature.pt\", weights_only = True).to(DEVICE)\n",
    "    real_tensor = torch.load(f\"../Data/Features/wst3/{cls}/nature_2.pt\", weights_only = True).to(DEVICE)\n",
    "    gt_tensor = torch.nn.functional.adaptive_avg_pool2d(gt_tensor, output_size=(16, 16))\n",
    "    real_tensor = torch.nn.functional.adaptive_avg_pool2d(real_tensor, output_size=(16, 16))\n",
    "    gt_tensor = utils.wst_m(gt_tensor, proj_g)\n",
    "    real_tensor = utils.wst_m(real_tensor, proj_g)\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\"]:\n",
    "        fake_tensor = torch.load(f\"../Data/Features/wst3/{cls}/{generator}.pt\", weights_only = True).to(DEVICE)\n",
    "        fake_tensor = torch.nn.functional.adaptive_avg_pool2d(fake_tensor, output_size=(16, 16))\n",
    "        fake_tensor = utils.wst_m(fake_tensor, proj_g)\n",
    "        labels = np.concatenate((np.zeros(real_tensor.shape[0]), np.ones(fake_tensor.shape[0])))\n",
    "        scores = mahalanobis_detector(gt_tensor, real_tensor, fake_tensor)\n",
    "        m = Metrics(labels, scores)\n",
    "        r,p,f = m.computation()\n",
    "        data[\"CLASS\"].append(cls)\n",
    "        data[\"GENERATOR\"].append(generator)\n",
    "        data[\"AUROC\"].append(r)\n",
    "        data[\"AUPRC\"].append(p)\n",
    "        data[\"FPR95\"].append(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"results_v2/wst3_gaussian_mahalanobis.csv\", index=False)\n",
    "print(f\"wst3 gaussian mahalanobis auroc: {np.mean(data['AUROC'])}, auprc: {np.mean(data['AUPRC'])}, fpr95: {np.mean(data['FPR95'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WST2VIT Mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WSTDinov2 Encoding:   0%|          | 0/3 [00:00<?, ?it/s]d:\\IPTP\\Stage_3A\\Stage\\Code\\.venv\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:81: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.69it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 388.26it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 431.58it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 419.95it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 433.12it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 416.30it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 430.03it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 423.74it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 431.91it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 431.92it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 422.14it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 440.08it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 421.55it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 414.75it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 437.92it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 425.71it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 427.86it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 413.38it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 432.89it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 426.77it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 427.81it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 422.52it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 411.82it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 434.87it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 426.93it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 435.35it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 419.28it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 416.96it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 412.75it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 429.02it/s]\n",
      "WSTDinov2 Encoding: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 433.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wst2dinov2 mahalanobis auroc: 0.5155438703449677, auprc: 0.530614766708069, fpr95: 0.9125514403292183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dinov2_model = AutoModel.from_pretrained(\"facebook/dinov2-with-registers-base\").to(DEVICE)\n",
    "data = {\"CLASS\": [], \"GENERATOR\": [], \"AUROC\": [], \"AUPRC\": [], \"FPR95\": []}\n",
    "\n",
    "for cls in classes_idx:\n",
    "    proj_s = SparseProjector(81, 48)\n",
    "\n",
    "    gt_tensor = torch.load(f\"../Data/Features/wst2/{cls}/nature.pt\", weights_only = True).to(DEVICE)\n",
    "    real_tensor = torch.load(f\"../Data/Features/wst2/{cls}/nature_2.pt\", weights_only = True).to(DEVICE)\n",
    "    gt_tensor = utils.wst_m(gt_tensor, proj_s, False)\n",
    "    real_tensor = utils.wst_m(real_tensor, proj_s, False)\n",
    "    gt_tensor = utils.reshape_to_3(gt_tensor)\n",
    "    real_tensor = utils.reshape_to_3(real_tensor)\n",
    "    gt_tensor = dinov2_encoder(gt_tensor, dinov2_model)\n",
    "    real_tensor = dinov2_encoder(real_tensor, dinov2_model)\n",
    "    os.makedirs(f\"../Data/Features/wst2dinov2/{cls}\", exist_ok=True)\n",
    "    torch.save(gt_tensor.detach().cpu(), f\"../Data/Features/wst2dinov2/{cls}/nature.pt\")\n",
    "    torch.save(real_tensor.detach().cpu(), f\"../Data/Features/wst2dinov2/{cls}/nature_2.pt\")\n",
    "\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\"]:\n",
    "        fake_tensor = torch.load(f\"../Data/Features/wst2/{cls}/{generator}.pt\", weights_only = True).to(DEVICE)\n",
    "        fake_tensor = utils.wst_m(fake_tensor, proj_s, False)\n",
    "        fake_tensor = utils.reshape_to_3(fake_tensor)\n",
    "        fake_tensor = dinov2_encoder(fake_tensor, dinov2_model)\n",
    "        torch.save(fake_tensor.detach().cpu(), f\"../Data/Features/wst2dinov2/{cls}/{generator}.pt\")\n",
    "\n",
    "        labels = np.concatenate((np.zeros(real_tensor.shape[0]), np.ones(fake_tensor.shape[0])))\n",
    "        scores = mahalanobis_detector(gt_tensor, real_tensor, fake_tensor)\n",
    "        m = Metrics(labels, scores)\n",
    "        r,p,f = m.computation()\n",
    "        data[\"CLASS\"].append(cls)\n",
    "        data[\"GENERATOR\"].append(generator)\n",
    "        data[\"AUROC\"].append(r)\n",
    "        data[\"AUPRC\"].append(p)\n",
    "        data[\"FPR95\"].append(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"results_v2/wst2dinov2_mahalanobis.csv\", index=False)\n",
    "print(f\"wst2dinov2 mahalanobis auroc: {np.mean(data['AUROC'])}, auprc: {np.mean(data['AUPRC'])}, fpr95: {np.mean(data['FPR95'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 18.73it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 24.37it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.96it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 488.18it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.85it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 509.02it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 24.36it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 526.92it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 22.84it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.98it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.70it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 516.70it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.75it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 519.96it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.37it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 514.02it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 22.86it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.72it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.73it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 491.44it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.41it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 500.35it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 24.63it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 509.49it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.32it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 24.59it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.84it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 511.25it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.31it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 513.33it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.85it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 520.90it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 22.81it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 24.33it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.62it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 503.71it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.68it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 516.67it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.49it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 504.54it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 22.44it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 24.74it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.78it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 491.43it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.73it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 494.66it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 22.93it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 512.03it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 22.93it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 24.63it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.47it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 518.93it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.25it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 493.79it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.91it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 475.23it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 22.46it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 24.08it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.68it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 492.08it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.23it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 473.84it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.73it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 501.63it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.54it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.77it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.51it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 506.07it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.98it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 494.35it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 24.22it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 492.57it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 22.31it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 24.19it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.87it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 487.75it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.64it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 505.87it/s]\n",
      "WSTClip Encoding: 100%|██████████| 3/3 [00:00<00:00, 23.68it/s]\n",
      "Mahalanobis scoring:: 100%|██████████| 324/324 [00:00<00:00, 490.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wst2clip mahalanobis auroc: 0.5798379312096734, auprc: 0.5807227509781118, fpr95: 0.8676954732510289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = {\"CLASS\": [], \"GENERATOR\": [], \"AUROC\": [], \"AUPRC\": [], \"FPR95\": []}\n",
    "clip_model, _ = clip.load(\"ViT-B/32\", device=DEVICE)\n",
    "\n",
    "for cls in classes_idx:\n",
    "    proj_s = SparseProjector(81, 48)\n",
    "\n",
    "    gt_tensor = torch.load(f\"../Data/Features/wst2/{cls}/nature.pt\", weights_only = True).to(DEVICE)\n",
    "    real_tensor = torch.load(f\"../Data/Features/wst2/{cls}/nature_2.pt\", weights_only = True).to(DEVICE)\n",
    "    gt_tensor = utils.wst_m(gt_tensor, proj_s, False)\n",
    "    real_tensor = utils.wst_m(real_tensor, proj_s, False)\n",
    "    gt_tensor = utils.reshape_to_3(gt_tensor)\n",
    "    real_tensor = utils.reshape_to_3(real_tensor)\n",
    "    gt_tensor = clip_encoder(gt_tensor, clip_model)\n",
    "    real_tensor = clip_encoder(real_tensor, clip_model)\n",
    "    os.makedirs(f\"../Data/Features/wst2clip/{cls}\", exist_ok=True)\n",
    "    torch.save(gt_tensor.detach().cpu(), f\"../Data/Features/wst2clip/{cls}/nature.pt\")\n",
    "    torch.save(real_tensor.detach().cpu(), f\"../Data/Features/wst2clip/{cls}/nature_2.pt\")\n",
    "\n",
    "    for generator in [\"bgan\", \"midj\", \"sd_15\"]:\n",
    "        fake_tensor = torch.load(f\"../Data/Features/wst2/{cls}/{generator}.pt\", weights_only = True).to(DEVICE)\n",
    "        fake_tensor = utils.wst_m(fake_tensor, proj_s, False)\n",
    "        fake_tensor = utils.reshape_to_3(fake_tensor)\n",
    "        fake_tensor = clip_encoder(fake_tensor, clip_model)\n",
    "        torch.save(fake_tensor.detach().cpu(), f\"../Data/Features/wst2clip/{cls}/{generator}.pt\")\n",
    "\n",
    "        labels = np.concatenate((np.zeros(real_tensor.shape[0]), np.ones(fake_tensor.shape[0])))\n",
    "        scores = mahalanobis_detector(gt_tensor, real_tensor, fake_tensor)\n",
    "        m = Metrics(labels, scores)\n",
    "        r,p,f = m.computation()\n",
    "        data[\"CLASS\"].append(cls)\n",
    "        data[\"GENERATOR\"].append(generator)\n",
    "        data[\"AUROC\"].append(r)\n",
    "        data[\"AUPRC\"].append(p)\n",
    "        data[\"FPR95\"].append(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"results_v2/wst2clip_mahalanobis.csv\", index=False)\n",
    "print(f\"wst2clip mahalanobis auroc: {np.mean(data['AUROC'])}, auprc: {np.mean(data['AUPRC'])}, fpr95: {np.mean(data['FPR95'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
